---
title: "Untitled"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(rvest)
library(htmltools)
library(here)
library(glue)
library(janitor)
source(here::here("Functions","drag_family_html_functions.R"))

wiki_root_url <- "https://rupaulsdragrace.fandom.com/wiki/"

```


```{r function test, echo=FALSE, include=FALSE}
drag_data <- get_drag_family_data(wiki_root_url)
```

# All Drag Families 

## Getting the data together 

Combined the root url with the Queens by drag house url and 
followed the pattern to extract the list of all url elements and 
tunred it into a data frame that to be combined with the rest of teh data 

this is an empty page
Not elegant, need to do some more reading on how to handle and empty element.
So far this has only happened for the house of stupi... hoping it is a one off
might have to make this a running list.... or maybe =check out the results
one second..
gotta do the same for the template page... just empty
```{r}
drag_house_url <- glue("{wiki_root_url}Category:Queens_by_Drag_House")

drag_house_results <- read_html(drag_house_url)

drag_house_names <- drag_house_results %>% 
  html_nodes(".category-page__member-link") %>% 
  html_text() 

drag_house_urls <-  drag_house_results %>% 
  html_nodes(".category-page__member") %>% 
  html_text("href")%>% 
  str_replace_all( "[\t\n]" , "") %>% 
  str_replace_all(" ","_") %>% # cleaning up the URLs 
  as.data.frame() %>% 
  rename("url_end"=1)

drag_house_frame <- tibble(wiki_root_url,drag_house_urls) %>% 
  mutate(family_url=as.character(glue("{wiki_root_url}{url_end}"))
         , family_name=str_replace(url_end,"Category:","")
         , family_name=str_replace_all(family_name,"_"," ")
         ) %>% 
  filter(!(url_end %in% c("Category:House_of_Stupi","Category:Template_(Family_Tree)"))
         ) %>% 
  mutate( html_results = map( family_url
                             , ~ {.x %>%
                                     read_html()
                                  }
                             )
         , all_names=map( html_results
                        , ~{.x %>% 
                              html_nodes(".category-page__member") %>% 
                              html_text() %>% 
                              str_replace_all( "[\t\n]" , "")
                            }
                       )
         ) %>% 
  select(-html_results) %>% 
  unnest(all_names) 
```

```{r save the base results for good measure, echo=FALSE, include=FALSE}
saveRDS(drag_house_frame,here::here("Output","drag_house_html_scrape_results.RDS"))

```


```{r Look for queens with more than one drag family, echo=FALSE, include=TRUE}
# Remove all scraping portions of the code and then create a cleaned up frame that 
# can be used for creating a node graph
# 
# Add in flag to id if a queen is a member of more than one family
# 
# I will figure out better naming conventions when the frames are loaded into 
# the in memory database
drag_family_connections <- drag_house_frame %>% 
  select(family_name, all_names) %>% 
  group_by(all_names) %>% 
  mutate(  mutliple_families=n()
         , multiple_family_flg=case_when(mutliple_families>1~1,
                                         TRUE~0)
         )%>% 
  ungroup()%>%
  select(-mutliple_families)

# Going to join this with the season data and create a flag for 
# having a fellow drag sister on the show. Can be used as model input
# Maybe there is a link of having a drag mother from a past season place high 

```

```{r scrape all drag race wikipedia pages for wikitables, echo=FALSE, include=FALSE}
url_frame <- data.frame(season=seq(14)) %>% 
  mutate(
          base_url="https://en.wikipedia.org/wiki/RuPaul%27s_Drag_Race_(season_"
        )

season_level_tables <- url_frame %>% 
  mutate(  season_url=glue("{base_url}{season})")
         , season_tables=map(season_url, ~ { 
                                              # .x is the dataframe piped in 
                                              # and for each row of the frame the 
                                              # chain of functions is executed 
                                              # a cleaner way of getthing things 
                                              # than going down the loop route
                                             .x %>% 
                                               read_html(season_url) %>% 
                                               html_nodes("table.wikitable") %>% 
                                               html_table(header=TRUE)  
                                            }
                                        )
         ) %>% 
  select(season, season_tables) %>% 
  unnest(season_tables) %>% 
  mutate(  season=as.character(season)
         , nested_col_names=map(season_tables, ~{
                                                  .x %>% 
                                                    names()
                                                }
                               )
         # Still need go get a better grasp on the whole mapping thing
         #, season_tables=map(season_tables ,~mutate(season_tables,season=season))
  )%>% # Season 3 has incomplete data no need to bring it in
  filter(str_detect(nested_col_names,"View"), season!=3)


```


```{r cleaning up the columns, echo=FALSE, include=FALSE }
viewership <- bind_rows(split(season_level_tables$season_tables,season_level_tables$season )) %>% 
  select(  `Episode`=`No.`
         , `Title`
         , `Rating`=`Rating(18â€“49)`
         , `Viewers`=`Viewers(millions)`
         ) %>%
  # Extract the digits and then convert to doubles
  mutate(  `Rating`=substr(`Rating`,1,4)
         , `Viewers`=substr(`Viewers`,1,5) 
         ) %>% 
  mutate_at(c("Rating","Viewers"), as.double) %>% 
  mutate(
            `Viewers`=`Viewers`*1000000
  )
  
  
```


```{r getting all episode level data, echo=FALSE, include=FALSE}
episode_url="https://en.wikipedia.org/wiki/List_of_RuPaul%27s_Drag_Race_episodes"

season_level_episodes <-map_df(episode_url, ~ { 
                                                       .x %>% 
                                                         read_html(episode_url) %>% 
                                                         html_node("table.wikitable") %>% 
                                                         html_table(header=TRUE)
                                                        }
                                        
          ) %>% 
   select(2 , 4) %>% 
   janitor::row_to_names(row=1) %>% 
   rename(  "season"="Season"
          , "episodes"="Episodes" )
```

```{r}
# this takes al of thetable.wiki objects and stacks them together 
# Probably should have done this sooner but whatever 
episode_tables <-map_dfr(episode_url, ~ { 
                                                       .x %>% 
                                                         read_html(episode_url) %>% 
                                                         html_nodes("table.wikitable") %>% 
                                                         html_table(header=TRUE)
                                                        }
                     ) %>% 
  filter(!is.na(`No.overall`)) %>% 
  select(`No.overall`:`Original air date`)

```

















```{r}
library(rvest)
# Reading in the table from Wikipedia
page = read_html("https://en.wikipedia.org/wiki/List_of_U.S._states_by_life_expectancy")
# Obtain the piece of the web page that corresponds to the "wikitable" node
my.table = html_node(page, ".wikitable")
# Convert the html table element into a data frame
my.table = html_table(my.table, fill = TRUE)
# Extracting and tidying a single column from the table and adding row names
x = as.numeric(gsub("\\[.*","",my.table[,4]))
names(x) = gsub("\\[.*","",my.table[,2])
# Excluding non-states and averages from the table
life.expectancy = x[!names(x) %in% c("United States", "Northern Mariana Islands", "Guam", "American Samoa", "Puerto Rico", "U.S. Virgin Islands")]
```

