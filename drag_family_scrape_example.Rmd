---
title: "Untitled"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(rvest)
library(htmltools)
library(here)
library(glue)
library(janitor)
source(here::here("Functions","drag_family_html_functions.R"))

wiki_root_url <- "https://rupaulsdragrace.fandom.com/wiki/"

```


```{r function test, echo=FALSE, include=FALSE}
drag_data <- get_drag_family_data(wiki_root_url)
```

# All Drag Families 

## Getting the data together 

Combined the root url with the Queens by drag house url and 
followed the pattern to extract the list of all url elements and 
tunred it into a data frame that to be combined with the rest of teh data 

this is an empty page
Not elegant, need to do some more reading on how to handle and empty element.
So far this has only happened for the house of stupi... hoping it is a one off
might have to make this a running list.... or maybe =check out the results
one second..
gotta do the same for the template page... just empty
```{r}
drag_house_url <- glue("{wiki_root_url}Category:Queens_by_Drag_House")

drag_house_results <- read_html(drag_house_url)

drag_house_names <- drag_house_results %>% 
  html_nodes(".category-page__member-link") %>% 
  html_text() 

drag_house_urls <-  drag_house_results %>% 
  html_nodes(".category-page__member") %>% 
  html_text("href")%>% 
  str_replace_all( "[\t\n]" , "") %>% 
  str_replace_all(" ","_") %>% # cleaning up the URLs 
  as.data.frame() %>% 
  rename("url_end"=1)

drag_house_frame <- tibble(wiki_root_url,drag_house_urls) %>% 
  mutate(family_url=as.character(glue("{wiki_root_url}{url_end}"))
         , family_name=str_replace(url_end,"Category:","")
         , family_name=str_replace_all(family_name,"_"," ")
         ) %>% 
  filter(!(url_end %in% c("Category:House_of_Stupi","Category:Template_(Family_Tree)"))
         ) %>% 
  mutate( html_results = map( family_url
                             , ~ {.x %>%
                                     read_html()
                                  }
                             )
         , all_names=map( html_results
                        , ~{.x %>% 
                              html_nodes(".category-page__member") %>% 
                              html_text() %>% 
                              str_replace_all( "[\t\n]" , "")
                            }
                       )
         ) %>% 
  select(-html_results) %>% 
  unnest(all_names) 
```

```{r save the base results for good measure, echo=FALSE, include=FALSE}
saveRDS(drag_house_frame,here::here("Output","drag_house_html_scrape_results.RDS"))

```


```{r Look for queens with more than one drag family, echo=FALSE, include=TRUE}
# Remove all scraping portions of the code and then create a cleaned up frame that 
# can be used for creating a node graph
# 
# Add in flag to id if a queen is a member of more than one family
# 
# I will figure out better naming conventions when the frames are loaded into 
# the in memory database
drag_family_connections <- drag_house_frame %>% 
  select(family_name, all_names) %>% 
  group_by(all_names) %>% 
  mutate(  mutliple_families=n()
         , multiple_family_flg=case_when(mutliple_families>1~1,
                                         TRUE~0)
         )%>% 
  ungroup()%>%
  select(-mutliple_families)

# Going to join this with the season data and create a flag for 
# having a fellow drag sister on the show. Can be used as model input
# Maybe there is a link of having a drag mother from a past season place high 

```

```{r}
url_frame <- data.frame(season=seq(14)) %>% 
  mutate(
            base_url="https://en.wikipedia.org/wiki/RuPaul%27s_Drag_Race_(season_"
          , episode_url="https://en.wikipedia.org/wiki/List_of_RuPaul%27s_Drag_Race_episodes")
```



```{r}
url_frame <- data.frame(season=seq(14)) %>% 
  mutate(
            base_url="https://en.wikipedia.org/wiki/RuPaul%27s_Drag_Race_(season_"
          )

season_level_tables <- url_frame %>% 
  mutate(  season_url=glue("{base_url}{season})")
                    , season_tables=map(season_url, ~ { 
                                                       .x %>% 
                                                         read_html(season_url) %>% 
                                                         html_nodes("table.wikitable") %>% 
                                                         html_table(header=TRUE)
                                                        }
                                        )
         ) %>% 
  select(season, season_tables) %>% 
  unnest(season_tables) %>% 
  mutate(df_rows=map_dbl(season_tables,~{
                                .x %>% 
                                nrow() 
                        })
         , season=as.character(season)
         , `Frame Names`=map(season_tables, ~{.x %>% 
                                      names()
                                    }
                    )
  )%>% 
  filter(str_detect(`Frame Names`,"View"))


```

```{r}
episode_url="https://en.wikipedia.org/wiki/List_of_RuPaul%27s_Drag_Race_episodes"

series_level_episodes <-map_df(episode_url, ~ { 
                                                       .x %>% 
                                                         read_html(episode_url) %>% 
                                                         html_node("table.wikitable") %>% 
                                                         html_table(header=TRUE)
                                                        }
                                        
          ) %>% 
   select(2 , 4) %>% 
   janitor::row_to_names(row=1) %>% 
   rename(  "season"="Season"
          , "episodes"="Episodes" )
```


```{r}
viewer_test <-season_level_tables %>% 
  left_join(series_level_episodes, by=c("season")) %>% 
  mutate(`Row Equiality Check(for viewers)`=case_when(df_rows==episodes~1
                                                      , TRUE~-0
                                                      )
         ) %>% 
  filter(`Row Equiality Check(for viewers)`==1) %>% 
  mutate() 
```


```{r}
# This will always be the first table from the wikipedia 
episode_frame<- url_frame$season_episodes[[1]]
```

```{r}


drag_wiki <- "https://en.wikipedia.org/wiki/RuPaul%27s_Drag_Race_(season_13)"

wiki_data <- read_html(drag_wiki) %>% 
  html_nodes("table.wikitable") %>% 
  html_table(header=TRUE)
# can loop through and use the frame that is the same size as the number of episodes in the number of rows
```




















```{r}
library(rvest)
# Reading in the table from Wikipedia
page = read_html("https://en.wikipedia.org/wiki/List_of_U.S._states_by_life_expectancy")
# Obtain the piece of the web page that corresponds to the "wikitable" node
my.table = html_node(page, ".wikitable")
# Convert the html table element into a data frame
my.table = html_table(my.table, fill = TRUE)
# Extracting and tidying a single column from the table and adding row names
x = as.numeric(gsub("\\[.*","",my.table[,4]))
names(x) = gsub("\\[.*","",my.table[,2])
# Excluding non-states and averages from the table
life.expectancy = x[!names(x) %in% c("United States", "Northern Mariana Islands", "Guam", "American Samoa", "Puerto Rico", "U.S. Virgin Islands")]
```

